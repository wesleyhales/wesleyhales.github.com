<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Selenium Grid on Wesley Hales</title>
    <link>http://wesleyhales.com/tags/selenium-grid/</link>
    <description>Recent content in Selenium Grid on Wesley Hales</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Sep 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://wesleyhales.com/tags/selenium-grid/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Browser Automation At Scale - Part 2</title>
      <link>http://wesleyhales.com/posts/2017-09-03-Browser-Automation-At-Scale-Part-2/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://wesleyhales.com/posts/2017-09-03-Browser-Automation-At-Scale-Part-2/</guid>
      <description>

&lt;p&gt;In &lt;a href=&#34;http://wesleyhales.com/posts/2017-08-07-Browser-Automation-At-Scale-Part-1/&#34;&gt;Part 1&lt;/a&gt;, I reviewed the gory details of the foundation required to run synthetic browser testing at scale.
Now that we have a framework for building out our tests, we can move forward with wrapping
our test runner in a web application so that the metrics we care about can be gathered and viewed through a decent UI.&lt;/p&gt;

&lt;p&gt;For this example, I&amp;rsquo;m going to use the Alexa top 12 news sites and execute a test over each one with the latest Chrome web browser. It will
open the page and grab the resource timing information along with a screenshot as shown here:
&lt;img src=&#34;http://wesleyhales.com/images/posts/2017-09-03/site_runner.png&#34; alt=&#34;&#34; /&gt;
The &lt;a href=&#34;https://github.com/wesleyhales/site_runner&#34;&gt;demo web application&lt;/a&gt; that we&amp;rsquo;re about to setup will store the above screenshots and timing information in Postgres.
The screenshots are base64 and a bit large compression wise, but that issue can be conquered at a later date. For now, all we
care about is to get things running and to scale our tests into every geography with a datacenter that supports CoreOS.
Let&amp;rsquo;s review the core architecture:
&lt;img src=&#34;http://wesleyhales.com/images/posts/2017-09-03/swarm_setup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;setting-up-site-runner&#34;&gt;Setting up site_runner&lt;/h3&gt;

&lt;p&gt;If you followed along in &lt;a href=&#34;http://wesleyhales.com/posts/2017-08-07-Browser-Automation-At-Scale-Part-1/&#34;&gt;Part 1&lt;/a&gt;, you can use your own Docker Swarm infrastructure to run your tests. For this
article, I&amp;rsquo;m only going to review the simple web application used to run and manage the tests - &lt;a href=&#34;https://github.com/wesleyhales/site_runner&#34;&gt;site_runner&lt;/a&gt;.
If you haven&amp;rsquo;t already, go ahead and clone the repository. You can run this within your local docker environment or
on another hosted CoreOS instance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone git@github.com:wesleyhales/site_runner.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;database-setup&#34;&gt;Database Setup&lt;/h4&gt;

&lt;p&gt;Next, we&amp;rsquo;ll setup a Postgres database. This will be used to store all of our test data and screenshots (in JSON format and queryable).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 5432:5432 --name siterunner_pg -e POSTGRES_PASSWORD=mysecretpassword -d postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to login to the database and set things up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -it --rm --link siterunner_pg:postgres postgres psql -h postgres -U postgres
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will run the Postgres client and will ask for a password (mysecretpassword).
After successful login, copy and paste the following and hit enter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE DATABASE SITERUNNER;
\c siterunner;
CREATE EXTENSION citext;
  CREATE TABLE timingdata (
    id serial primary key,
    data jsonb,
    image text,
    email citext unique
  );
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;web-application-setup&#34;&gt;Web Application Setup&lt;/h4&gt;

&lt;p&gt;The site_runner web application is a custom test runner built to give simple reporting and an API for the tests
 that need to be ran. Open a new terminal window and navigate to the repository you cloned at the beginning of this article and run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; docker build -t wesleyhales/site_runner .
 docker run -p 3000:3000 --link siterunner_pg:postgres -d wesleyhales/site_runner
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will start your application at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;. Visit the URL and ensure you see a page with a link and some table headers.
There won&amp;rsquo;t be any data to review yet, which brings us to the next step&amp;hellip;&lt;/p&gt;

&lt;p&gt;The site_runner demo application
uses /delete to clear the database and /startTest to start the tests (pretty simple, right?). startTest requires a query parameter
for which region you&amp;rsquo;d like your tests to be ran.  If you followed along in Part 1, you would use sf1-node as your query paramter argument.
For this article, I&amp;rsquo;ve added another node in New York, so an HTTP request to the following will kick off your tests in that region:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:3000/startTest?nodeName=ny3-node&#34;&gt;http://localhost:3000/startTest?nodeName=ny3-node&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If everything goes as planned, you will see the following response from hitting that URL:
&lt;img src=&#34;http://wesleyhales.com/images/posts/2017-09-03/test_response.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s also get a tail on our web server logs so you can see the messages as each test runs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker logs -f [your container ID]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When each test finishes (and you refresh the page), you&amp;rsquo;ll see the screenshot and some other simple data about when and where the
test was ran.
&lt;img src=&#34;http://wesleyhales.com/images/posts/2017-09-03/test_result.png&#34; alt=&#34;&#34; /&gt;
If tests aren&amp;rsquo;t running, check the Selenium hub &lt;a href=&#34;http://165.227.123.79:4444/grid/console&#34;&gt;console page&lt;/a&gt; to see how many requests are currently queued. Or stop being lazy and go setup
everything in &lt;a href=&#34;http://wesleyhales.com/posts/2017-08-07-Browser-Automation-At-Scale-Part-1/&#34;&gt;Part 1&lt;/a&gt; and manage your own testing infrastructure! ;-)&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re just following along and don&amp;rsquo;t have time to set all this up, you can view a temporary site_runner report &lt;a href=&#34;http://165.227.123.79:3000/?account=Alexa%20Top%2012%20News&#34;&gt;here&lt;/a&gt;.
Hover over the screenshot to get a zoomed in view and examine any visual defects.
(It would be cool to get &lt;a href=&#34;https://huddle.github.io/Resemble.js/&#34;&gt;Resemble.js&lt;/a&gt; integrated to handle image analysis and comparison at some point)&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like to change the test sites, you can do so in &lt;a href=&#34;https://github.com/wesleyhales/site_runner/blob/master/model/AllSites.js&#34;&gt;this file&lt;/a&gt;.
Other things you might want to change are the data that is harvested during each page run. Here, in
&lt;a href=&#34;https://github.com/wesleyhales/site_runner/blob/master/selenium/siterunner.js#L55&#34;&gt;siterunner.js&lt;/a&gt;, you can see where
I&amp;rsquo;ve written a few wait commands that eventually execute JavaScript on the page that is loaded. This allows you
to harvest any required metrics or inspect things to see what is being used on the page.&lt;/p&gt;

&lt;p&gt;In Part 3, We&amp;rsquo;ll advance our webdriver script to crawl an entire site, add a few more nodes across the globe and look at a few open source and pay for monitoring tools that
 will give us health visibility into our nodes and testing infrastructure. Part 4 will probably go into adding headless Chrome and Firefox to the mix.&lt;/p&gt;

&lt;p&gt;Thanks to Andy Davies for the &lt;a href=&#34;https://github.com/andydavies/waterfall&#34;&gt;ResourceTiming waterfall component&lt;/a&gt;!
&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Browser Automation At Scale - Part 1</title>
      <link>http://wesleyhales.com/posts/2017-08-07-Browser-Automation-At-Scale-Part-1/</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://wesleyhales.com/posts/2017-08-07-Browser-Automation-At-Scale-Part-1/</guid>
      <description>

&lt;h4 id=&#34;docker-swarm-and-selenium&#34;&gt;Docker Swarm and Selenium&lt;/h4&gt;

&lt;p&gt;Both Docker and Selenium are pretty much household names these days in the world of software engineering. I&amp;rsquo;ve been fascinated
with Docker since its inception and have been using it for side projects and in my day job for a few years now.
I recently came across the need to test a Chrome extension and load a web page while that extension is installed.
This test would load the page, wait for it to load, check some JS variables and APIs and then spit out a screenshot
and any needed metrics. In the end, each test would run from multiple geographies and produce a report on performance
and visual defects to be viewed at any point in time.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the End-to-End Architecture:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker Swarm underlying connectivity across all nodes&lt;/li&gt;
&lt;li&gt;Selenium (Grid) network with single centralized hub for test queuing&lt;/li&gt;
&lt;li&gt;Selenium nodes per geography. Dedicated 4GB (minimum) CoreOS instance for Chrome sessions managed by Selenium.&lt;/li&gt;
&lt;li&gt;Navigation and Resource timing data along with Screenshots stored in linked postgres DB container.&lt;/li&gt;
&lt;li&gt;Centralized web application to manage test reporting and test data storage.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For this first installment, we&amp;rsquo;re going to setup Selenium Grid on top of Docker Swarm, run a test and get some visibility into the DOM
and how things work. The connectivity and orchestration will be managed by Swarm and Selenium will open the browser
(Chrome for this example) and run our tests.
What I&amp;rsquo;m about to show you, in this and upcoming installments, is extremely powerful and should only be used for good legitimate browser testing
against sites you own or are responsible for ;-)
I&amp;rsquo;m going to scale everything back to the simplest
setup possible and we&amp;rsquo;ll expand for world domination in later tutorials.&lt;/p&gt;

&lt;p&gt;Other (Headless) browsers such as Firefox, PhantomJS and Headless Chrome can be plugged in or substituted at any time.
All we need to worry about is to make sure the tests are written using the WebDriver API.&lt;/p&gt;

&lt;h4 id=&#34;setup-docker-swarm-along-with-the-selenium-grid-hub-and-node&#34;&gt;Setup Docker Swarm along with the Selenium Grid, Hub and Node&lt;/h4&gt;

&lt;p&gt;There are four main components to the underlying testing infrastructure, which I quickly summarize below. The good news
is that there are no configuration files to write and everything is used out of box from the official Docker repositories
of each project. The goal of my setup is to be as disposable as possible. Everything is to be reproducible without any
glue code or unnecessary abstraction layers.&lt;/p&gt;

&lt;h5 id=&#34;quick-component-summary&#34;&gt;Quick Component Summary&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;For Docker Swarm information, check out &lt;a href=&#34;https://docs.docker.com/engine/swarm/&#34;&gt;the docs&lt;/a&gt; for the full rundown. It&amp;rsquo;s the underlying
networking, provisioning, and orchestration layer for docker containers.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.seleniumhq.org/docs/07_selenium_grid.jsp&#34;&gt;Selenium Grid&lt;/a&gt; allows you run your tests on different machines against
different browsers in parallel. A grid consists of a single hub, and one or more nodes, Hub and Node are the two main elements
that you come across when creating a grid.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://hub.docker.com/r/selenium/hub/&#34;&gt;Selenium Hub&lt;/a&gt; instance will find an available node that matches the criteria we send in with our test parameters.
Once it finds a machine that matches a browser version you want to run against, the hub reroutes the test to the desired node.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://hub.docker.com/r/selenium/node-chrome/&#34;&gt;Selenium Node&lt;/a&gt; used for this example is Google Chrome.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;swarm-and-hub-machine-setup&#34;&gt;Swarm and Hub - Machine Setup&lt;/h4&gt;

&lt;p&gt;The first thing you need to do is login to your favorite hosting provider that can provision CoreOS instances. For the hub
I&amp;rsquo;ve been using a 1 processor 2GB CoreOS machine. In my testing, I ran into problems with anything less that 2GB of memory for the hub.&lt;/p&gt;

&lt;p&gt;SSH into the newly created instance and run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker swarm init --advertise-addr [your external IP]
docker network create -d overlay selenium-grid
docker service create --network selenium-grid --name hub -p 4444:4444 selenium/hub
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Here we&amp;rsquo;re initializing Docker Swarm, the Selenium Grid networking layer and the Selenium Hub Service.&lt;/li&gt;
&lt;li&gt;Using this machine&amp;rsquo;s IP address, visit this url (http://[your ip]:4444/grid/console#) to view the Selenium Grid Console. If you cant see it,
something is wrong. And debugging this setup is a whole other blog post.&lt;/li&gt;
&lt;li&gt;Leave this terminal window open. We&amp;rsquo;ll be back in a sec.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;selenium-node-machine-setup&#34;&gt;Selenium Node - Machine Setup&lt;/h3&gt;

&lt;p&gt;Now we&amp;rsquo;ll setup the first Chrome Node that will connect to the Hub. You&amp;rsquo;ll need to create another CoreOS VM, this time with
a minimum of 2 processors and 4GB of memory. Also, make a note of the hostname of this machine by using the following command.
The hostname is normally defined within the hosting provider&amp;rsquo;s UI. If not, you can can change it (if needed) once you SSH into the instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; hostnamectl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to join this Node to the Swarm.
The join command was given to you when you executed docker init above. If you lost it,
run &lt;code&gt;docker swarm join-token worker&lt;/code&gt; from the hub instance again and copy and paste it into this Node instance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker swarm join --token [token] [ip]:[port]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it for this machine. Everything else will be orchestrated from the hub (until part two of this series
where I show you how to install a Chrome extension on the fly)&lt;/p&gt;

&lt;h3 id=&#34;selenium-node-service-deployment&#34;&gt;Selenium Node - Service Deployment&lt;/h3&gt;

&lt;p&gt;Go back into the main hub instance from the first step. Let&amp;rsquo;s deploy the Selenium Node Docker service. This will start the
Selenium Chrome client on the Node machine you just setup. Remember everything will be deployed and managed from the hub.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker service create --network selenium-grid --name selenium-node-chrome-sfo --constraint &#39;node.hostname==sfo1-node-01&#39; -p 5560:5560 --mount type=bind,src=/dev/shm,dst=/dev/shm -e HUB_PORT_4444_TCP_ADDR=hub -e HUB_PORT_4444_TCP_PORT=4444 -e NODE_MAX_INSTANCES=1 -e NODE_MAX_SESSION=1 --replicas 1 selenium/node-chrome bash -c &#39;SE_OPTS=&amp;quot;-browser applicationName=sfo1-node,browserName=chrome,maxInstances=1 -host $HOSTNAME -port 5560&amp;quot; /opt/bin/entry_point.sh&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a hefty command and you might be wondering what all of the switched and parameters are for. I&amp;rsquo;ll attempt to break it
 down for you here:&lt;/p&gt;

&lt;p&gt;1) &lt;code&gt;node.hostname==sfo1-node-01&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is the hostname of the actual CoreOS instance. We&amp;rsquo;re using it to constrain the selenium environment to only run on this CoreOS vm.
This way, we won&amp;rsquo;t have to worry about other tests running in parallel and messing with memory or CPU usage. This is meant to be a pristine,
white glove environment.&lt;/li&gt;
&lt;li&gt;If you&amp;rsquo;re creating an instance for another region, you&amp;rsquo;ll want to replace &amp;lsquo;sfo&amp;rsquo; above with that region&amp;rsquo;s airport or country code.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) &lt;code&gt;SE_OPTS=-browser applicationName=sfo1-node&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This parameter gives our test script a hook so that it will run on this specific SFO node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) &lt;code&gt;--name selenium-node-chrome-sfo&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is just a name to identify this machine within docker. Should be kept in some kind of naming convention order.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4) &lt;code&gt;--mount type=bind,src=/dev/shm,dst=/dev/shm&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This command forces Docker to use the host&amp;rsquo;s memory. When Chrome continually (or randomly) crashed during test runs, this command seemed to solve the issue.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5) &lt;code&gt;-e NODE_MAX_INSTANCES=1 -e NODE_MAX_SESSION=1 --replicas 1&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;the-test-script&#34;&gt;The Test Script&lt;/h4&gt;

&lt;p&gt;To get started, we&amp;rsquo;ll setup some basic logging so we can see the test execute and hopefully use it for debugging purposes later.
&lt;script src=&#34;https://gist.github.com/wesleyhales/ca2b8f8844061b4b4d70c82ecb744ea3.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server logging basically tells you when the test execution began and when it ended.&lt;/li&gt;
&lt;li&gt;Performance Logging gives us more information around frame navigation. We&amp;rsquo;ll be able to correlate this later with navigation and Resource timing APIs.&lt;/li&gt;
&lt;li&gt;For all loggin configuration options see &lt;a href=&#34;https://github.com/SeleniumHQ/selenium/wiki/Logging&#34;&gt;https://github.com/SeleniumHQ/selenium/wiki/Logging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following snippet, we&amp;rsquo;re adding more criteria for our test and actually sending log messages that we setup earlier to console.log:
&lt;script src=&#34;https://gist.github.com/wesleyhales/83d2ae26df603b1b08c74cfbb8c8fee8.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first few lines specify which node we&amp;rsquo;re going to run this test on. Later we&amp;rsquo;ll parameterize this to handle more regions.&lt;/li&gt;
&lt;li&gt;This is followed by the actual WebDriver initialization and setup. Make sure you add your hub IP address from the first step above.&lt;/li&gt;
&lt;li&gt;Finally you see the console statements for the native WebDriver logging.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And finally, we get to the meat of this exercise:
&lt;script src=&#34;https://gist.github.com/wesleyhales/ee1a3d298830d6dbc50fb0a8be6b6a1d.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First we setup the screen size for the browser window and then actually perform the GET request.&lt;/li&gt;
&lt;li&gt;Then we see the driver.wait command. I&amp;rsquo;m using this a a fail safe to ensure the browser has time to populate the Resource Timing entries for perf measurements.&lt;/li&gt;
&lt;li&gt;Next we take a screenshot and save it to the filesystem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When this script executes, you&amp;rsquo;ll see all of our logging entries print out to the main terminal window. Since all
this data is in JSON format, it makes it easy for us to setup a database and store it on each run. In the next article
I&amp;rsquo;ll show you how to wrap this script in a node web application and store the JSON in Postgres (along with the screenshot) on each run.
In Part 3 we&amp;rsquo;ll look at scaling out to support more regions, look at why nodes seem to fail randomly, and how to add multiple
nodes from different hosting providers e.g. AWS, Digital Ocean, etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>